{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PW2: Doing a soup with Beautiful Soup\n",
    "\n",
    "## Beginning\n",
    "\n",
    "### [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "\n",
    "# Quests\n",
    "- Scrap one table and put the result in a CSV\n",
    "- Scrap another table and using Pandas (PW1) to manipulate those data (plotting)\n",
    "- Scrap all the table in the page \n",
    "\n",
    "\n",
    "## Target\n",
    "\n",
    "We'll try to scrap data from Dehli MCD Election from this URL http://www.elections.in/delhi/mcd-elections/\n",
    "\n",
    "\n",
    "## Importing librairies\n",
    "Here, we are going to use 3 differents librairies :\n",
    "- `urllib` to go to the webpage and get all the page (GET request)\n",
    "- `BeautifulSoup` for reading the HTML file\n",
    "- `csv` to write the result inside a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import bs4 from BeautifulSoup\n",
    "\n",
    "# import urllib --> librairy for request the website\n",
    "\n",
    "# import csv --> librairy for manipulating csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the web page\n",
    "\n",
    "Our data are in webpage, organized with HTML markups which we can see in web browser. We'll download the webpage and parse it to get data.\n",
    "\n",
    "### Docs:\n",
    "* urllib request: https://docs.python.org/3.4/library/urllib.request.html#module-urllib.request\n",
    "* BeautifulSoup Quickstart: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start\n",
    "\n",
    "\n",
    "### Download data\n",
    "\n",
    " 1. Perform a request to the specific URL and put the HTML page inside a variable.\n",
    " 2. Parse HTML data received before\n",
    " 3. Display title of the page\n",
    " \n",
    "#### Do it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put URL in a variable\n",
    "DEHLI_MCD_URL = 'http://www.elections.in/delhi/mcd-elections/'\n",
    "# Open URL with urllib and read data into a variable\n",
    "\n",
    "# Parse HTML variable with BeautifulSoup() method and put it in a variable\n",
    "\n",
    "# Use print function to display \"title\" field of the webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "You can choose any markup you want in this page. For that you to stay close of the \"code source view\" of the page to know which explicit markup you want to scrap.\n",
    "\n",
    "### Docs\n",
    "* BeaufifulSoup find functions: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#calling-a-tag-is-like-calling-find-all\n",
    "\n",
    "\n",
    "### Scrap it !\n",
    "\n",
    " 1. Display the number of `<table>` statement in the page\n",
    " 2. Scrap the table \"Delhi Municipal Corporation Wards and Seats Reservation\" (which is the seventh `<table>` in HTML)\n",
    " 3. Display the data\n",
    "\n",
    "#### Do it yourself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find all the <table> markups in the HTML with findAll() function and put them in a variable\n",
    "\n",
    "# Print the number of tables in the page with len() method\n",
    "\n",
    "# Get the seventh table using previous variable (careful: tables's ids start from 0). Put it in a variable.\n",
    "\n",
    "# Find all rows (<tr> markups>) of the table, using the variable created before\n",
    "\n",
    "# Optionnal: Print raw data to verify what you have scrapped. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export your data\n",
    "\n",
    "With those data you can export them in a file to work with your favorite software. We'll see how to export them to a CSV file which a simple format and can be used with many softwares.\n",
    "\n",
    "### Docs\n",
    "\n",
    "* Manipulate CSV file: https://docs.python.org/2/library/csv.html\n",
    "* Python Data structures: https://docs.python.org/3.4/library/urllib.request.html#module-urllib.request\n",
    "\n",
    "### Writing CSV file\n",
    "\n",
    "1. Open a CSV file with write rule\n",
    "2. Write data in the file\n",
    "\n",
    "\n",
    "#### Do it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open a CSV file with open() function\n",
    "\n",
    "    # Use csv library to open writer handle with opened file\n",
    "\n",
    "    # Make a loop to extract rows data in tuple, then write it to handle   \n",
    "\n",
    "\n",
    "\n",
    "# Check your file in Jupyter's work folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
